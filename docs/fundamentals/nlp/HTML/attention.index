<br><br>
    <div id="attention">
        <h1>Attention</h1>

        <div id="attention_intro">
            <h2>The Problem of Fixed Encoder Representation</h2>
            <div class="green_left_thought" style="font-size:18px;">
                <p class="data_text"><u>Problem</u>: Fixed source representation
                    is suboptimal: (i) for the encoder, it is hard to compress the sentence; (ii)
                    for the decoder, different information may be relevant at different steps.
                </p>
            </div>

            <img src="../resources/lectures/seq2seq/attention/bottleneck-min.png"
             style="max-width:60%; margin:20px;float:right;"/>

            <p>In the models we looked at so far, the encoder compressed the whole source sentence into a single
                vector. This can be very hard - the number of possible
                source sentences (hence, their meanings) is infinite. When the encoder is forced to put all information into
                a single vector, it is likely to forget something.
            </p>
            <p><font class="data_text" color="#888"><u>Lena</u>: Imagine
                    the whole universe in all its beauty - try to visualize everything you can find there and how you can
                    describe it in words.
                    Then imagine all of it is compressed into a single vector of size e.g. 512. Do you feel that
                    the universe is still ok?
                </font>
            </p>

            <p>Not only it is hard for the encoder to put all information into a single vector - this is also
                hard for the decoder. The decoder sees only one representation of source. However, at each
                generation step, different parts of source can be more useful than others. But in the current setting,
                the decoder has to extract relevant information from the same fixed representation -
                hardly an easy thing to do.
            </p>

        </div>



        <div id="attention_idea">
            <h2>Attention: A High-Level View</h2>

            <p>Attention was introduced in the paper
                <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">
                    Neural Machine Translation by Jointly Learning to Align and Translate
                </a> to address the fixed representation problem. </p>
            <div class="green_left_thought" style="font-size:18px;">
                <p class="data_text"><u>Attention</u>: At different steps,
                    let a model "focus" on different parts of the input.
                </p>
            </div>

            <p>An attention mechanism is a part of a neural network. At each decoder step,
                it decides which source parts are more important.
                In this setting,
                the encoder does not have to compress the whole source into a single vector -
                it gives representations for all
                source tokens (for example, all RNN states instead of the last one).
            </p>

            <img src="../resources/lectures/seq2seq/attention/general_scheme-min.png"
             style="max-width:100%; margin:20px;margin-top:0px;"/>
            <p>At each decoder step, attention</p>
            <ul>
                <li>receives <font face="arial">attention input</font>: a decoder state \(\color{#b01277}{h_t}\) and all encoder states
                    \(\color{#7fb32d}{s_1}\), \(\color{#7fb32d}{s_2}\), ..., \(\color{#7fb32d}{s_m}\);</li>
                <li>computes <font face="arial">attention scores</font><br>
                    For each encoder state \(\color{#7fb32d}{s_k}\), attention computes its "relevance" for
                    this decoder state \(\color{#b01277}{h_t}\). Formally, it applies
                    an attention function which receives one decoder state and one encoder state and returns a
                    scalar value \(\color{#307cc2}{score}(\color{#b01277}{h_t}\color{black}{,}\color{#7fb32d}{s_k}\color{black})\);
                </li>
                <li>computes <font face="arial">attention weights</font>:
                    a probability distribution - softmax applied to attention scores;</li>
                <li>computes <font face="arial">attention output</font>:
                    the weighted sum of encoder states with attention weights.</li>
            </ul>

            <p>The general computation scheme is shown below.</p>
            <img src="../resources/lectures/seq2seq/attention/computation_scheme-min.png"
             style="max-width:80%; margin:20px;"/>

            <h3>Note: Everything is differentiable - learned end-to-end!</h3>
            <p>The main idea that a network can <font face="arial">learn</font> which input parts
                are more important at each step.
                Since everything here is differentiable (attention function, softmax, and all the rest),
                a model with attention can be trained end-to-end. You don't need to specifically teach the model
                to pick the words you want -
                <font face="arial">the model itself will learn to pick important information</font>.
            </p>


        <br>
        <img height="20" src="../resources/lectures/ico/paw_empty.png" style="float:left; margin-top:-10px;"/>
        <div class="box_green_left">

            <div class="text_box_green">
              <p class="data_text"><u>How to:</u> go over the slides at your pace. Try to notice how
                  attention weights change from step to step - which words are the most important at each step?</p>
            </div>

            <div class="carousel" data-flickity='{ "imagesLoaded": true, "percentPosition": true,
            "selectedAttraction": 1, "friction": 1 }'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/1-min.png"/></center>
              </div>
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/2-min.png"/></center>
              </div>
                <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/3-min.png"/></center>
              </div>
                <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/4-min.png"/></center>
              </div>
                <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/5-min.png"/></center>
              </div>
                <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/6-min.png"/></center>
              </div>
                <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/7-min.png"/></center>
              </div>
                <div class="carousel-cell" style="width:100%"><center>
                    <img width=100% src="../resources/lectures/seq2seq/attention/attn_for_steps/8-min.png"/></center>
              </div>

            </div>

        </div>
        <img height="20" src="../resources/lectures/ico/paw_empty.png" style="float:left; margin-top:-10px;"/>
        <br><br>


        </div>

        <div id="attention_specific_functions">


            <img src="../resources/lectures/seq2seq/attention/attn_score_what_is_here-min.png"
             style="max-width:35%; margin-left:20px; float:right;"/>

            <h2> How to Compute Attention Score?</h2>
            <p>In the general pipeline above, we haven't specified how exactly we compute
                attention scores. You can apply any function you want - even a very complicated one.
                However, usually you don't need to - there are several popular and simple variants which work quite well.
            </p>

            <center>
            <img src="../resources/lectures/seq2seq/attention/score_functions-min.png"
             style="max-width:90%; margin-bottom:20px;"/>
            </center>

            <p>The most popular ways to compute attention scores are:</p>
            <ul>
                <li><font face="arial">dot-product</font> - the simplest method;</li>
                <li><font face="arial">bilinear function</font> (aka "Luong attention") - used in the paper
                    <a href="https://arxiv.org/abs/1508.04025" target="_blank">Effective
                        Approaches to Attention-based Neural Machine Translation</a>;</li>
                <li><font face="arial">multi-layer perceptron</font> (aka "Bahdanau attention") -
                    the method proposed in the
                    <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">original paper</a>.
                    </li>
            </ul>



        </div>




        <div id="attention_bahdanau_luong">



            <h2> Model Variants: Bahdanau and Luong </h2>

            <p>When talking about the early attention models, you are most likely to hear
                these variants:</p>
            <ul>
                <li><font face="arial">Bahdanau attention</font> - from the paper <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">
                    Neural Machine Translation by Jointly Learning to Align and Translate
                </a> by Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio (this is the paper that introduced the attention
                mechanism for the first time);
                </li>
                <li><font face="arial">Luong attention</font> - from the paper <a href="https://arxiv.org/abs/1508.04025" target="_blank">Effective
                        Approaches to Attention-based Neural Machine Translation</a> by
                Minh-Thang Luong, Hieu Pham, Christopher D. Manning.
                </li>
            </ul>

            <p>These may refer to either score functions of the whole models used in these papers.
                In this part, we will look more
                closely at these two model variants.
            </p>


            <h3> <u>Bahdanau Model</u> </h3>
            <ul><li><font face="arial">encoder: bidirectional</font><br>
            To better encode each source word, the encoder has two RNNs, forward and backward,
                which read input in the opposite directions. For each token, states of the two RNNs are concatenated.
            </li>
                <li><font face="arial">attention score: multi-layer perceptron</font><br>
                    To get an attention score, apply a multi-layer perceptron (MLP) to
                    an encoder state and a decoder state.
                </li>
                <li><font face="arial">attention applied: between decoder steps</font><br>
                    Attention is used between decoder steps: state \(\color{#b01277}{h_{t-1}}\) is used to
                    compute attention and its output \(\color{#7fb32d}{c}^{\color{#b01277}{(t)}}\), and
                     both \(\color{#b01277}{h_{t-1}}\) and \(\color{#7fb32d}{c}^{\color{#b01277}{(t)}}\)
                    are passed to the decoder at step \(t\).
                </li>

            </ul>

            <center>
            <img src="../resources/lectures/seq2seq/attention/bahdanau_model-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
            </center>


            <h3> <u>Luong Model</u> </h3>
            <p>While the <a href="https://arxiv.org/abs/1508.04025" target="_blank">paper</a> considers several model variants,
            the one which is usually called "Luong attention" is the following:
            </p>
            <ul><li><font face="arial">encoder: unidirectional</font> (simple)
            </li>
                <li><font face="arial">attention score: bilinear function</font>
                </li>
                <li><font face="arial">attention applied: between decoder RNN state \(t\) and prediction for this step</font><br>
                    Attention is used after RNN decoder step \(t\) before making a prediction.
                    State \(\color{#b01277}{h_{t}}\) used to
                    compute attention and its output \(\color{#7fb32d}{c}^{\color{#b01277}{(t)}}\). Then
                    \(\color{#b01277}{h_{t}}\) is combined with \(\color{#7fb32d}{c}^{\color{#b01277}{(t)}}\)
                    to get an updated representation \(\color{#b01277}{\tilde{h}_{t}}\),
                    which is used to get a prediction.
                </li>

            </ul>

            <center>
            <img src="../resources/lectures/seq2seq/attention/luong_model-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
            </center>



        </div>

        <div id="attention_alignment">

            <h2>Attention Learns (Nearly) Alignment</h2>

            <p>Remember the motivation for attention? At different steps,
                the decoder may need to focus on different
            source tokens, the ones which are more relevant at this step. Let's look at attention weights -
                which source words does the decoder use?
            </p>

            <p style="text-align: center; display: block;
            margin-bottom:20px; max-width:100%;">
           <img src="../resources/lectures/seq2seq/attention/bahdanau_examples-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
                <br />
            <span style="font-size: small;">The examples are from the paper
            <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">Neural Machine Translation
                by Jointly Learning to Align and Translate</a>.</span>
            </p>

            <p>From the examples, we see that attention
                learned (soft) alignment between source and target words - the decoder looks at those source
                tokens which it is translating at the current step.
            </p>

            <p class="data_text"><font color="#888"><u>Lena</u>:
                "Alignment" is a term from
                statistical machine translation, but in this part, its intuitive understanding as
                "what is translated to what" is enough.
            </font></p>


        </div>


    </div>